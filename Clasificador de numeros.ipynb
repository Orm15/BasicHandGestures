{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presiona 'q' para empezar/pausar el guardado de coordenadas.\n",
      "Presiona 'ESC' para salir.\n",
      "Guardando coordenadas...\n",
      "Guardado pausado.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Inicializaci√≥n de MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Configuraci√≥n del modelo\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=1,  # Solo detectar una mano\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "\n",
    "# Captura de video (c√°mara web)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Archivo CSV para guardar las coordenadas\n",
    "output_file = \"coordenadas_mano_numero6.csv\"\n",
    "\n",
    "# Crear encabezados para el CSV (x1, y1, z1, ..., x21, y21, z21)\n",
    "header = [f\"{coord}{i+1}\" for i in range(21) for coord in (\"x\", \"y\", \"z\")]\n",
    "\n",
    "# Crear o sobrescribir el archivo CSV\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "print(\"Presiona 'q' para empezar/pausar el guardado de coordenadas.\")\n",
    "print(\"Presiona 'ESC' para salir.\")\n",
    "\n",
    "# Bandera para iniciar/pausar el guardado\n",
    "save_coordinates = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"No se pudo acceder a la c√°mara.\")\n",
    "        break\n",
    "\n",
    "    # Convertir la imagen a RGB (MediaPipe utiliza im√°genes RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Procesar la imagen para detectar manos\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Verificar si se detect√≥ una mano\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]  # Solo la primera mano detectada\n",
    "\n",
    "        # Dibujar las landmarks de la mano en la imagen\n",
    "        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Obtener coordenadas y guardarlas si est√° habilitado\n",
    "        if save_coordinates:\n",
    "            # Obtener todas las coordenadas x, y, z en una lista\n",
    "            coords = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                coords.extend([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            # Guardar las coordenadas en el archivo CSV\n",
    "            with open(output_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(coords)\n",
    "\n",
    "    # Mostrar la imagen con las detecciones\n",
    "    cv2.imshow('Seguimiento de Mano', frame)\n",
    "\n",
    "    # Control de teclas\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        save_coordinates = not save_coordinates\n",
    "        if save_coordinates:\n",
    "            print(\"Guardando coordenadas...\")\n",
    "        else:\n",
    "            print(\"Guardado pausado.\")\n",
    "    elif key == 27:  # Tecla 'ESC' para salir\n",
    "        break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA est√° disponible\n",
      "üîπ GPU detectada: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ CUDA est√° disponible\")\n",
    "    print(f\"üîπ GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA no est√° disponible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìå 1. Cargar los 5 CSVs y asignar etiquetas\n",
    "csv_files = {\n",
    "    \"coordenadas_mano_numero1.csv\": 0,\n",
    "    \"coordenadas_mano_numero2.csv\": 1,\n",
    "    \"coordenadas_mano_numero3.csv\": 2,\n",
    "    \"coordenadas_mano_numero4.csv\": 3,\n",
    "    \"coordenadas_mano_numero5.csv\": 4,\n",
    "}\n",
    "\n",
    "data_list = []\n",
    "for file, label in csv_files.items():\n",
    "    df = pd.read_csv(file, header=None)  # No hay encabezados\n",
    "    df[\"label\"] = label  # Agregar la etiqueta de clase\n",
    "    data_list.append(df)\n",
    "\n",
    "data = pd.concat(data_list, ignore_index=True)  # Combinar todos los archivos\n",
    "\n",
    "# üìå 2. Convertir datos en tensores\n",
    "X = data.iloc[:, :-1].values.astype(np.float32)  # 63 entradas\n",
    "y = data.iloc[:, -1].values.astype(np.int64)  # Etiquetas (1-5)\n",
    "\n",
    "# üìå 3. Dividir en entrenamiento y validaci√≥n (80%-20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìå 4. Crear Dataset en PyTorch\n",
    "class HandGestureDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = HandGestureDataset(X_train, y_train)\n",
    "val_dataset = HandGestureDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# üìå 5. Definir la Red Neuronal\n",
    "class HandGestureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandGestureModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(63, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 5)  # 5 clases\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìå 6. Inicializar el Modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HandGestureModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Epoch [1/30], Loss: 0.6630\n",
      "üîπ Epoch [2/30], Loss: 0.1520\n",
      "üîπ Epoch [3/30], Loss: 0.0811\n",
      "üîπ Epoch [4/30], Loss: 0.0569\n",
      "üîπ Epoch [5/30], Loss: 0.0505\n",
      "üîπ Epoch [6/30], Loss: 0.0474\n",
      "üîπ Epoch [7/30], Loss: 0.0379\n",
      "üîπ Epoch [8/30], Loss: 0.0441\n",
      "üîπ Epoch [9/30], Loss: 0.0361\n",
      "üîπ Epoch [10/30], Loss: 0.0392\n",
      "üîπ Epoch [11/30], Loss: 0.0336\n",
      "üîπ Epoch [12/30], Loss: 0.0297\n",
      "üîπ Epoch [13/30], Loss: 0.0365\n",
      "üîπ Epoch [14/30], Loss: 0.0288\n",
      "üîπ Epoch [15/30], Loss: 0.0288\n",
      "üîπ Epoch [16/30], Loss: 0.0336\n",
      "üîπ Epoch [17/30], Loss: 0.0307\n",
      "üîπ Epoch [18/30], Loss: 0.0262\n",
      "üîπ Epoch [19/30], Loss: 0.0338\n",
      "üîπ Epoch [20/30], Loss: 0.0203\n",
      "üîπ Epoch [21/30], Loss: 0.0271\n",
      "üîπ Epoch [22/30], Loss: 0.0242\n",
      "üîπ Epoch [23/30], Loss: 0.0290\n",
      "üîπ Epoch [24/30], Loss: 0.0285\n",
      "üîπ Epoch [25/30], Loss: 0.0252\n",
      "üîπ Epoch [26/30], Loss: 0.0258\n",
      "üîπ Epoch [27/30], Loss: 0.0300\n",
      "üîπ Epoch [28/30], Loss: 0.0272\n",
      "üîπ Epoch [29/30], Loss: 0.0299\n",
      "üîπ Epoch [30/30], Loss: 0.0216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üìå 7. Entrenar la Red Neuronal\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"üîπ Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Precisi√≥n en validaci√≥n: 99.50%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üìå 8. Evaluar el Modelo\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"üîπ Precisi√≥n en validaci√≥n: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo guardado como 'modelo_gestos.pth'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üìå 9. Guardar el Modelo\n",
    "torch.save(model.state_dict(), \"modelo_gestos.pth\")\n",
    "print(\"‚úÖ Modelo guardado como 'modelo_gestos.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con porcentajes sobre la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_18628\\2768951722.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"modelo_gestos.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# üìå Definir la Red Neuronal\n",
    "class HandGestureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandGestureModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(63, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# üìå Cargar el modelo entrenado\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = HandGestureModel().to(device)\n",
    "model.load_state_dict(torch.load(\"modelo_gestos.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# üìå Inicializar MediaPipe para detecci√≥n de manos\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# üìå Captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "class_labels = [\"numero 1\", \"numero 2\", \"numero 3\", \"numero 4\", \"numero 5\"]  # Nombres de las clases\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"No se pudo acceder a la c√°mara.\")\n",
    "        break\n",
    "\n",
    "    # Convertir a RGB para MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # üìå Dibujar la mano y extraer coordenadas\n",
    "    coords = None\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Obtener coordenadas x, y, z\n",
    "            coords = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "\n",
    "    # üìå Realizar predicci√≥n si hay coordenadas\n",
    "    if coords is not None and len(coords) == 63:\n",
    "        input_tensor = torch.tensor(coords, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        # Obtener la clase con mayor probabilidad\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        predicted_label = class_labels[predicted_class]\n",
    "\n",
    "        # üìå Dibujar cuadro con las probabilidades\n",
    "        bar_x = frame.shape[1] - 200\n",
    "        bar_y = 50\n",
    "        bar_width = 150\n",
    "        bar_height = 20\n",
    "\n",
    "        for i, (label, prob) in enumerate(zip(class_labels, probabilities)):\n",
    "            bar_fill = int(prob * bar_width)  # Escalar al tama√±o de la barra\n",
    "            cv2.rectangle(frame, (bar_x, bar_y + i * 30), (bar_x + bar_fill, bar_y + i * 30 + bar_height), (0, 255, 0), -1)\n",
    "            cv2.rectangle(frame, (bar_x, bar_y + i * 30), (bar_x + bar_width, bar_y + i * 30 + bar_height), (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"{label}: {prob:.2f}\", (bar_x + 5, bar_y + i * 30 + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "        # üìå Mostrar el resultado en pantalla\n",
    "        cv2.putText(frame, f\"Prediccion: {predicted_label}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "    # üìå Mostrar la imagen con detecciones\n",
    "    cv2.imshow('Detectar numero de dedos', frame)\n",
    "\n",
    "    # Salir con 'ESC'\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# üìå Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con panel a la derecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_18628\\2421628171.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"modelo_gestos.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# üìå Definir la Red Neuronal\n",
    "class HandGestureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandGestureModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(63, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 5)  # 5 clases\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# üìå Cargar el modelo entrenado\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HandGestureModel().to(device)\n",
    "model.load_state_dict(torch.load(\"modelo_gestos.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# üìå Inicializar MediaPipe para detecci√≥n de manos\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# üìå Captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "class_labels = [\"numero 1\", \"numero 2\", \"numero 3\", \"numero 4\", \"numero 5\"]  # Nombres de las clases\n",
    "\n",
    "# üìå Tama√±o del panel negro\n",
    "panel_width = 250  # Ancho del panel de informaci√≥n\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"No se pudo acceder a la c√°mara.\")\n",
    "        break\n",
    "\n",
    "    # Convertir a RGB para MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # üìå Dibujar la mano y extraer coordenadas\n",
    "    coords = None\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Obtener coordenadas x, y, z\n",
    "            coords = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "\n",
    "    # üìå Crear el panel negro a la derecha\n",
    "    panel = np.zeros((frame.shape[0], panel_width, 3), dtype=np.uint8)\n",
    "\n",
    "    # üìå Realizar predicci√≥n si hay coordenadas\n",
    "    if coords is not None and len(coords) == 63:\n",
    "        input_tensor = torch.tensor(coords, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        # Obtener la clase con mayor probabilidad\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        predicted_label = class_labels[predicted_class]\n",
    "\n",
    "        # üìå Dibujar texto y barras en el panel negro\n",
    "        bar_x = 20\n",
    "        bar_y = 50\n",
    "        bar_width = panel_width - 40  # Ancho de la barra\n",
    "        bar_height = 30\n",
    "\n",
    "        for i, (label, prob) in enumerate(zip(class_labels, probabilities)):\n",
    "            bar_fill = int(prob * bar_width)  # Escalar al tama√±o de la barra\n",
    "            cv2.rectangle(panel, (bar_x, bar_y + i * 50), (bar_x + bar_fill, bar_y + i * 50 + bar_height), (0, 255, 0), -1)\n",
    "            cv2.rectangle(panel, (bar_x, bar_y + i * 50), (bar_x + bar_width, bar_y + i * 50 + bar_height), (255, 255, 255), 2)\n",
    "            cv2.putText(panel, f\"{label}: {prob:.2f}\", (bar_x + 5, bar_y + i * 50 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "        # üìå Mostrar el nombre del gesto detectado\n",
    "        cv2.putText(panel, f\"Prediccion:\", (bar_x, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(panel, predicted_label, (bar_x + 100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # üìå Concatenar la imagen y el panel negro\n",
    "    combined_frame = np.hstack((frame, panel))\n",
    "\n",
    "    # üìå Mostrar la imagen con detecciones y el panel de informaci√≥n\n",
    "    cv2.imshow('Deteccion de numeros', combined_frame)\n",
    "\n",
    "    # Salir con 'ESC'\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# üìå Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
