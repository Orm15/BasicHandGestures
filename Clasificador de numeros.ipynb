{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presiona 'q' para empezar/pausar el guardado de coordenadas.\n",
      "Presiona 'ESC' para salir.\n",
      "Guardando coordenadas...\n",
      "Guardado pausado.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Inicialización de MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Configuración del modelo\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=1,  # Solo detectar una mano\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "\n",
    "# Captura de video (cámara web)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Archivo CSV para guardar las coordenadas\n",
    "output_file = \"coordenadas_mano_numero6.csv\"\n",
    "\n",
    "# Crear encabezados para el CSV (x1, y1, z1, ..., x21, y21, z21)\n",
    "header = [f\"{coord}{i+1}\" for i in range(21) for coord in (\"x\", \"y\", \"z\")]\n",
    "\n",
    "# Crear o sobrescribir el archivo CSV\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "print(\"Presiona 'q' para empezar/pausar el guardado de coordenadas.\")\n",
    "print(\"Presiona 'ESC' para salir.\")\n",
    "\n",
    "# Bandera para iniciar/pausar el guardado\n",
    "save_coordinates = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"No se pudo acceder a la cámara.\")\n",
    "        break\n",
    "\n",
    "    # Convertir la imagen a RGB (MediaPipe utiliza imágenes RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Procesar la imagen para detectar manos\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Verificar si se detectó una mano\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]  # Solo la primera mano detectada\n",
    "\n",
    "        # Dibujar las landmarks de la mano en la imagen\n",
    "        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Obtener coordenadas y guardarlas si está habilitado\n",
    "        if save_coordinates:\n",
    "            # Obtener todas las coordenadas x, y, z en una lista\n",
    "            coords = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                coords.extend([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            # Guardar las coordenadas en el archivo CSV\n",
    "            with open(output_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(coords)\n",
    "\n",
    "    # Mostrar la imagen con las detecciones\n",
    "    cv2.imshow('Seguimiento de Mano', frame)\n",
    "\n",
    "    # Control de teclas\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        save_coordinates = not save_coordinates\n",
    "        if save_coordinates:\n",
    "            print(\"Guardando coordenadas...\")\n",
    "        else:\n",
    "            print(\"Guardado pausado.\")\n",
    "    elif key == 27:  # Tecla 'ESC' para salir\n",
    "        break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA está disponible\n",
      "🔹 GPU detectada: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA está disponible\")\n",
    "    print(f\"🔹 GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"❌ CUDA no está disponible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 📌 1. Cargar los 5 CSVs y asignar etiquetas\n",
    "csv_files = {\n",
    "    \"coordenadas_mano_numero1.csv\": 0,\n",
    "    \"coordenadas_mano_numero2.csv\": 1,\n",
    "    \"coordenadas_mano_numero3.csv\": 2,\n",
    "    \"coordenadas_mano_numero4.csv\": 3,\n",
    "    \"coordenadas_mano_numero5.csv\": 4,\n",
    "}\n",
    "\n",
    "data_list = []\n",
    "for file, label in csv_files.items():\n",
    "    df = pd.read_csv(file, header=None)  # No hay encabezados\n",
    "    df[\"label\"] = label  # Agregar la etiqueta de clase\n",
    "    data_list.append(df)\n",
    "\n",
    "data = pd.concat(data_list, ignore_index=True)  # Combinar todos los archivos\n",
    "\n",
    "# 📌 2. Convertir datos en tensores\n",
    "X = data.iloc[:, :-1].values.astype(np.float32)  # 63 entradas\n",
    "y = data.iloc[:, -1].values.astype(np.int64)  # Etiquetas (1-5)\n",
    "\n",
    "# 📌 3. Dividir en entrenamiento y validación (80%-20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 📌 4. Crear Dataset en PyTorch\n",
    "class HandGestureDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = HandGestureDataset(X_train, y_train)\n",
    "val_dataset = HandGestureDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 📌 5. Definir la Red Neuronal\n",
    "class HandGestureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandGestureModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(63, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 5)  # 5 clases\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 📌 6. Inicializar el Modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HandGestureModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Epoch [1/30], Loss: 0.6630\n",
      "🔹 Epoch [2/30], Loss: 0.1520\n",
      "🔹 Epoch [3/30], Loss: 0.0811\n",
      "🔹 Epoch [4/30], Loss: 0.0569\n",
      "🔹 Epoch [5/30], Loss: 0.0505\n",
      "🔹 Epoch [6/30], Loss: 0.0474\n",
      "🔹 Epoch [7/30], Loss: 0.0379\n",
      "🔹 Epoch [8/30], Loss: 0.0441\n",
      "🔹 Epoch [9/30], Loss: 0.0361\n",
      "🔹 Epoch [10/30], Loss: 0.0392\n",
      "🔹 Epoch [11/30], Loss: 0.0336\n",
      "🔹 Epoch [12/30], Loss: 0.0297\n",
      "🔹 Epoch [13/30], Loss: 0.0365\n",
      "🔹 Epoch [14/30], Loss: 0.0288\n",
      "🔹 Epoch [15/30], Loss: 0.0288\n",
      "🔹 Epoch [16/30], Loss: 0.0336\n",
      "🔹 Epoch [17/30], Loss: 0.0307\n",
      "🔹 Epoch [18/30], Loss: 0.0262\n",
      "🔹 Epoch [19/30], Loss: 0.0338\n",
      "🔹 Epoch [20/30], Loss: 0.0203\n",
      "🔹 Epoch [21/30], Loss: 0.0271\n",
      "🔹 Epoch [22/30], Loss: 0.0242\n",
      "🔹 Epoch [23/30], Loss: 0.0290\n",
      "🔹 Epoch [24/30], Loss: 0.0285\n",
      "🔹 Epoch [25/30], Loss: 0.0252\n",
      "🔹 Epoch [26/30], Loss: 0.0258\n",
      "🔹 Epoch [27/30], Loss: 0.0300\n",
      "🔹 Epoch [28/30], Loss: 0.0272\n",
      "🔹 Epoch [29/30], Loss: 0.0299\n",
      "🔹 Epoch [30/30], Loss: 0.0216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 📌 7. Entrenar la Red Neuronal\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"🔹 Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Precisión en validación: 99.50%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 📌 8. Evaluar el Modelo\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"🔹 Precisión en validación: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo guardado como 'modelo_gestos.pth'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 📌 9. Guardar el Modelo\n",
    "torch.save(model.state_dict(), \"modelo_gestos.pth\")\n",
    "print(\"✅ Modelo guardado como 'modelo_gestos.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con porcentajes sobre la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_18628\\2768951722.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"modelo_gestos.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 📌 Definir la Red Neuronal\n",
    "class HandGestureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandGestureModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(63, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 📌 Cargar el modelo entrenado\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = HandGestureModel().to(device)\n",
    "model.load_state_dict(torch.load(\"modelo_gestos.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 📌 Inicializar MediaPipe para detección de manos\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# 📌 Captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "class_labels = [\"numero 1\", \"numero 2\", \"numero 3\", \"numero 4\", \"numero 5\"]  # Nombres de las clases\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"No se pudo acceder a la cámara.\")\n",
    "        break\n",
    "\n",
    "    # Convertir a RGB para MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # 📌 Dibujar la mano y extraer coordenadas\n",
    "    coords = None\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Obtener coordenadas x, y, z\n",
    "            coords = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "\n",
    "    # 📌 Realizar predicción si hay coordenadas\n",
    "    if coords is not None and len(coords) == 63:\n",
    "        input_tensor = torch.tensor(coords, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        # Obtener la clase con mayor probabilidad\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        predicted_label = class_labels[predicted_class]\n",
    "\n",
    "        # 📌 Dibujar cuadro con las probabilidades\n",
    "        bar_x = frame.shape[1] - 200\n",
    "        bar_y = 50\n",
    "        bar_width = 150\n",
    "        bar_height = 20\n",
    "\n",
    "        for i, (label, prob) in enumerate(zip(class_labels, probabilities)):\n",
    "            bar_fill = int(prob * bar_width)  # Escalar al tamaño de la barra\n",
    "            cv2.rectangle(frame, (bar_x, bar_y + i * 30), (bar_x + bar_fill, bar_y + i * 30 + bar_height), (0, 255, 0), -1)\n",
    "            cv2.rectangle(frame, (bar_x, bar_y + i * 30), (bar_x + bar_width, bar_y + i * 30 + bar_height), (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"{label}: {prob:.2f}\", (bar_x + 5, bar_y + i * 30 + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "        # 📌 Mostrar el resultado en pantalla\n",
    "        cv2.putText(frame, f\"Prediccion: {predicted_label}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "    # 📌 Mostrar la imagen con detecciones\n",
    "    cv2.imshow('Detectar numero de dedos', frame)\n",
    "\n",
    "    # Salir con 'ESC'\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# 📌 Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con panel a la derecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_18628\\2421628171.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"modelo_gestos.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 📌 Definir la Red Neuronal\n",
    "class HandGestureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandGestureModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(63, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 5)  # 5 clases\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 📌 Cargar el modelo entrenado\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HandGestureModel().to(device)\n",
    "model.load_state_dict(torch.load(\"modelo_gestos.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 📌 Inicializar MediaPipe para detección de manos\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# 📌 Captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "class_labels = [\"numero 1\", \"numero 2\", \"numero 3\", \"numero 4\", \"numero 5\"]  # Nombres de las clases\n",
    "\n",
    "# 📌 Tamaño del panel negro\n",
    "panel_width = 250  # Ancho del panel de información\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"No se pudo acceder a la cámara.\")\n",
    "        break\n",
    "\n",
    "    # Convertir a RGB para MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # 📌 Dibujar la mano y extraer coordenadas\n",
    "    coords = None\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Obtener coordenadas x, y, z\n",
    "            coords = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "\n",
    "    # 📌 Crear el panel negro a la derecha\n",
    "    panel = np.zeros((frame.shape[0], panel_width, 3), dtype=np.uint8)\n",
    "\n",
    "    # 📌 Realizar predicción si hay coordenadas\n",
    "    if coords is not None and len(coords) == 63:\n",
    "        input_tensor = torch.tensor(coords, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        # Obtener la clase con mayor probabilidad\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        predicted_label = class_labels[predicted_class]\n",
    "\n",
    "        # 📌 Dibujar texto y barras en el panel negro\n",
    "        bar_x = 20\n",
    "        bar_y = 50\n",
    "        bar_width = panel_width - 40  # Ancho de la barra\n",
    "        bar_height = 30\n",
    "\n",
    "        for i, (label, prob) in enumerate(zip(class_labels, probabilities)):\n",
    "            bar_fill = int(prob * bar_width)  # Escalar al tamaño de la barra\n",
    "            cv2.rectangle(panel, (bar_x, bar_y + i * 50), (bar_x + bar_fill, bar_y + i * 50 + bar_height), (0, 255, 0), -1)\n",
    "            cv2.rectangle(panel, (bar_x, bar_y + i * 50), (bar_x + bar_width, bar_y + i * 50 + bar_height), (255, 255, 255), 2)\n",
    "            cv2.putText(panel, f\"{label}: {prob:.2f}\", (bar_x + 5, bar_y + i * 50 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "        # 📌 Mostrar el nombre del gesto detectado\n",
    "        cv2.putText(panel, f\"Prediccion:\", (bar_x, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(panel, predicted_label, (bar_x + 100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # 📌 Concatenar la imagen y el panel negro\n",
    "    combined_frame = np.hstack((frame, panel))\n",
    "\n",
    "    # 📌 Mostrar la imagen con detecciones y el panel de información\n",
    "    cv2.imshow('Deteccion de numeros', combined_frame)\n",
    "\n",
    "    # Salir con 'ESC'\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# 📌 Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
